{
    "contents" : "---\ntitle: \"Predicting type of Physical Exercises from Wearable Sensors\"\nauthor: \"Jiri Hron\"\ndate: \"Sunday, August 17, 2014\"\noutput:\n  html_document:\n    fig_caption: yes\n    fig_height: 6\n    fig_width: 9\n    keep_md: yes\n    number_sections: yes\n    theme: journal\n---\n\n# Synopsis\n\nThis paper was written as course assignment for Coursera's class __Practical\nMachine Learning__. Main goal of this assignment was to develop a predictive\nmodel, which would be able to predict type of physical exercise being performed\nby experiment participants based on the data collected from wearable motorical\nsensors attached to their bodies. In the following parts, a short description\nof what preprocessing was used to the original data is given, followed by\nelementary exploratory analysis. Last two chapters are then describing how\nthe final model was designed and picked, and out-of-sample estimate of \nthe algorithm accuracy is provided.\n\n# Data preprocessing\n\n```{r echo=FALSE, cache=TRUE, warning=FALSE, cache.vars=TRUE}\n## load a file from\nsource(file = \"../code/cleanData.R\", local = TRUE)\ndata = cleanData(\"../data/pml-training.csv\")\n\n## statistics mentioned in text\ncolCount = ncol(data)\nrowCount = nrow(data)\n\n## split data into training and test set\nset.seed(1123)\nproportionTrain = .75\ninTrain = createDataPartition(y = data$classe,\n                              p = proportionTrain, \n                              list = FALSE)\n\ntraining = data[inTrain,]\ntesting = data[-inTrain,]\n\n## create objects used later by predictive algorithms and for plotting (pca)\nmodel.mat = training[,!(colnames(training) %in% \"classe\")]\n\nmodel.scale = preProcess(x = model.mat, method = c(\"center\", \"scale\"))\nmodel.normalized = predict(model.scale, model.mat)\n\nmodel.pca = preProcess(x = model.normalized,\n                       method = \"pca\",\n                       thresh = 0.95)\nmodel.pca.projected = predict(model.pca, model.normalized)\n\n\nmodel.normalized$classe = training$classe\nmodel.pca.projected$classe = training$classe\n\n## create model for testing data\ntesting.model = testing[,!(colnames(testing) %in% \"classe\")]\ntesting.scaled = predict(model.scale, testing.model)\ntesting.pca = predict(model.pca, testing.scaled)\n```\n\nData set used for this assignment was obtained as a subset of original\ndata [here][1]. Before approaching model\ndesign, descriptive variables (*X*, *user_name*, *raw_timestamp_part_1*, \n*raw_timestamp_part_2*, *cvtd_timestamp*) and records containing aggregate\nstatistics for larger time window of previous rows in the data set (identified\nby variable value `new_window == \"yes\"`) were removed.  \n\nAll remaining variables were tested for completeness, i.e. if there is at least\none valid value in the whole data set. Even this weak criterion and previously\ndescribed steps reduced variable column space from 160 to `r colCount`\n(resp. from 159 to `r colCount - 1` if counting off `classe`).\n\nFull data set of `r rowCount` variables was divided into train and test sets,\nwith relative sizes of `r proportionTrain` and `r 1 - proportionTrain` for\nboth sets respectively.  \n\nFor purpose of exploratory analysis and model fitting, three versions of feature\nmodel were prepared:  \n\n  1. **raw** - no transformation, original values  \n  2. **normalized** - features were scaled and centered  \n  3. **pca** - principal components transformations, preserving 95% of original\n  variance\n\n# Data exploration\n\nBefore exploration of sample space, we need to assure that our predicted \nvariable `classe` (type of the activity being performed) is approximately\ndistributed or at least doesn't contain any very rare classes:\n\n``` {r echo=FALSE}\nsummary(training$classe)\n```\n\nNow we explore if any of variables are unnecessary in terms of very low\nvariance. Such variables have very low predictive power and could cause\nover-fitting of the model. As you can see on following figure, no such variables\nwere identified:\n\n```{r echo=FALSE, cache=TRUE, warning=FALSE}\nif(!require(caret)) {\n  stop(\"install caret package\")\n}\n\nnearZeroVar(x = model.mat, saveMetrics = TRUE)\n```\n\nSince our feature space is still `r colCount - 1`-dimensional, we will use pca\ntransformation to get a basic insight into the aggregate information contained\nin the data set. As already mentioned in previous text, principal components\nwere chosen as such to preserve 95% of the variance in the data, which yielded\n`r ncol(model.pca.projected)` principal directions. Even projection of data to\nthe first two of them shows separation of data into 5 subgroups. However, this\ndiscernible pattern does not correspond to a distribution of `classe` variable\npictured by color of the points.\n\n``` {r echo=FALSE, warning=FALSE, cache=TRUE, comment=FALSE, fig.cap=\"First two principal components\"}\nif(!(require(RColorBrewer))) {\n  stop(\"install RColorBrewer package\")\n}\n\n## plot first two principal components\npallete = brewer.pal(length(levels(model.pca.projected$classe)), \"Accent\")\n\ntheme = theme_set(theme_minimal())\nggplot(data = model.pca.projected, aes(x = PC1, y = PC2, col = classe)) +\n  geom_jitter(alpha = 0.6) + scale_color_manual(values = pallete)\n```\n\nIf we involve the third principal component, we could see that clusters from\nprevious figure are forming oblong clusters on the next figure (third principal\ncomponent is on vertical axis).\n\n``` {r echo=FALSE, warning=FALSE, cache=TRUE, comment=FALSE, fig.cap=\"First three principal components\"}\nif(!(require(scatterplot3d))) {\n  stop(\"install scatterplot3d package\")\n}\n\n## plot again with first three\ncol = pallete[as.numeric(model.pca.projected$classe)]\nwith(model.pca.projected, \n     scatterplot3d(x = PC1, y = PC2, z = PC3,\n                   pch = 20, angle = 100, color = col,\n                   xlab = \"first principal component\",\n                   ylab = \"second principal component\",\n                   zlab = \"third principal component\"))\nlegend(\"topright\",\n       pch = 20, yjust=0,\n       legend = levels(model.pca.projected$classe), \n       col = pallete)\n```\n\nSince PCA reduced the number of dimensions by \n`r ncol(model.pca.projected) * 100 / (colCount - 1)`% while still preserving 95%\nof original variance, we will use principal components transformation in order\nto speed up the learning process of our predictive algorithm.\n\nTo understand which of the original variables has the greatest influence on\nfirst three principal directions, we can inspect coefficients used to obtain\neach of the corresponding principal scores. In the next figure you can see\ntop 10 most influential coefficients for first principal direction ordered by\ntheir absolute value:\n\n```{r echo=FALSE}\nmodel.pca.prcomp = as.data.frame(model.pca$rotation)\nwith(model.pca.prcomp, \n     head(model.pca.prcomp[order(abs(PC1), decreasing = TRUE),][, 1:3]), 10)\n```\n\n# Prediction model development\n\nPurpose of following paragraphs is to design a best possible model, where best\nmeans with highest prediction **accuracy**.\n\n## Design\n\n```{r echo=FALSE, warning=FALSE, cache=TRUE, results='hide', comment=FALSE, message=FALSE}\nif(!(require(doParallel))) {\n  stop(\"install doParallel package\")\n}\n\nsource('../code/prepareTrainFunction.R')\n\n## run model in parallel\ncl = makeCluster(detectCores())\nregisterDoParallel(cl)\n\n# rrlda model\ncv.trainControl = prepareTrainFunction(9)\ngrid.rrlda = data.frame(lambda = c(0.01, 0.1, 0.3, 1, 3, 6, 10, 20, 30),\n                  hp = 0.75,\n                  penalty = \"L2\")\nfit.rrlda = train(classe ~ .,\n                  data = model.pca.projected,\n                  method = \"rrlda\",\n                  trControl = cv.trainControl,\n                  tuneGrid = grid.rrlda)\n\n\n# SVM model\ncv.trainControl = prepareTrainFunction(13)\ngrid.svm = data.frame(C = c(0.1, 1, 3, 5, 10, 15, 20, 30, 35, 40, 45, 50, 60))\nfit.svm = train(classe ~ .,\n                data = model.pca.projected,\n                method = \"svmRadialCost\",\n                trControl = cv.trainControl,\n                tuneGrid = grid.svm)\n\nstopCluster(cl)\n```\n\nIn the beginning we started by fitting two types of models, namely:\n\n* Robust Regularized Linear Discriminant Analysis\n* Support Vector Machines with RBF (Gaussian) Kernel\n\nThe former of those two models represents an algorithm preferring bias over\nvariance while using only linear fit, the latter one is on the contrary able\nto fit more general types of non-linear relationships between variables and\nthus might tend to prefer variance. However both can be tuned by user provided\nparameters, which can regularize the fit (increasing $\\lambda$ for rrlda,\ndecreasing __C__ for SVM) or improve its flexibility (increasing __C__ for SVM,\ndecreasing $\\lambda$ for rrlda).\n\n## Tuning\n\nFor both of the models, adaptive 10-fold cross-validation was used so as to \nfurther decrease the training time needed and to improve predictive accuracy. \nAdaptive cross-validation is at the time of writing this paper relatively new\napproach presented by [Kuhn (2014)][2] which dynamically reduces number of\ncandidate tuning parameters during cross-validation. Method used to rule-out\nfutile models used was the default one for _trainControl_ function used by\n_caret_, which uses generalized least squares as a measure of each tuning\nparameter's value \"fruitfulness\" and gets rid of any set of tuning parameters\nthat have significantly worse cv-error, than that of the best model at given\niteration (one-sided hypothesis test on 95% significant level was used to\ndetermine such models). Finally, first 5 cv-iterations were performed with\nfull set of potential tuning parameters and reduction of candidate parameters\nwas carried out at every following iteration, using average model error from\nall preceding iterations (refer to original paper for more details about\nthis method).  \nAdditional tuning parameters used for rrlda:\n\n```{r echo=FALSE}\nprint(grid.rrlda)\n```\n\nThe `hp` parameter is specifying proportion of available training examples\nwhich should be used for model fitting (robustness parameter), `penalty` is \ntype of the penalty to be used (we used L2 penalty which is more suitable\nfor environments where most of the predictors - in our cas pca transforms - are\nexpected to be significant predictors). $\\lambda$ is then the regularization\nparameter which we tried to find by by cross-validation.  \n\nAdditional tuning parameters used for SVM:\n\n```{r echo=FALSE}\nprint(grid.svm)\n```\n\nThe only tuning parameter for SVM was regularization constant __C__ \n(using C-svc type of SV). The $\\sigma$ for Gaussian kernel was estimated\ninternally by method `kernlab::sigest` from by cross-validation.\n\nIn the next chapter, summarized results for both of the models are provided,\nincluding the top parameters selected by cross-validation for both models.\n\n# Evaluation\n\nFollowing figure is showing confusion matrix for SVM model with RBF (Gaussian)\nkernel comparing model predictions with our testing set of `r nrow(testing)`\nobservations. Notice `Accuracy` line which is our final estimate for test error\naccuracy of the best SVM model (best model had __C__ = `r fit.svm$bestTune`).\n\n``` {r echo=FALSE, warning=FALSE, message=FALSE}\nif(!require(caret)) {\n  stop(\"install caret package\")\n}\nconfusionMatrix(testing$classe, predict(fit.svm, testing.pca))\n```\n\nNext, we assess performance of rrlda model. As you can see\non the following figure, `Accuracy` was not very good even for the best model\nwith parameters ($\\lambda$,hp,penalty) = (`r fit.rrlda$bestTune`).\n\n```{r echo=FALSE, message=FALSE, warning=FALSE}\nif(!require(caret)) {\n  stop(\"install caret package\")\n}\nconfusionMatrix(testing$classe, predict(fit.rrlda, testing.pca))\n```\n\nAs you can see, SVM model has superior test error estimate and since its value\nis very close to absolute optimum, there is no need for further tuning.  \n`Accuracy` estimation for SVM model is also our estimate for out-of-sample\nerror.\n\n[1]: http://groupware.les.inf.puc-rio.br/har\n[2]: http://arxiv.org/pdf/1405.6974v1.pdf \"Kuhn (2014)\"",
    "created" : 1408257752385.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3890545174",
    "id" : "CF6B2778",
    "lastKnownWriteTime" : 1408617378,
    "path" : "C:/Users/IBM_ADMIN/Desktop/education/academicworld/data_science/practical_ml/project/report/report.Rmd",
    "project_path" : "report/report.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}