---
title: "Predicting type of Physical Exercises from Wearable Sensors"
author: "Jiri Hron"
date: "Sunday, August 17, 2014"
output:
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 9
    keep_md: yes
    number_sections: yes
    theme: journal
---

# Synopsis

This paper was written as course assignment for Coursera's class __Practical
Machine Learning__. Main goal of this assignment was to develop a predictive
model, which would be able to predict type of physical exercise being performed
by experiment participants based on the data collected from wearable motorical
sensors attached to their bodies. In the following parts, a short description
of what preprocessing was used to the original data is given, followed by
elementary exploratory analysis. Last two chapters are then describing how
the final model was designed and picked, and out-of-sample estimate of 
the algorithm accuracy is provided.

# Data preprocessing

```{r echo=FALSE, cache=TRUE, warning=FALSE, cache.vars=TRUE}
## load a file from
source(file = "../code/cleanData.R", local = TRUE)
data = cleanData("../data/pml-training.csv")

## statistics mentioned in text
colCount = ncol(data)
rowCount = nrow(data)

## split data into training and test set
set.seed(1123)
proportionTrain = .75
inTrain = createDataPartition(y = data$classe,
                              p = proportionTrain, 
                              list = FALSE)

training = data[inTrain,]
testing = data[-inTrain,]

## create objects used later by predictive algorithms and for plotting (pca)
model.mat = training[,!(colnames(training) %in% "classe")]

model.scale = preProcess(x = model.mat, method = c("center", "scale"))
model.normalized = predict(model.scale, model.mat)

model.pca = preProcess(x = model.normalized,
                       method = "pca",
                       thresh = 0.95)
model.pca.projected = predict(model.pca, model.normalized)


model.normalized$classe = training$classe
model.pca.projected$classe = training$classe

## create model for testing data
testing.model = testing[,!(colnames(testing) %in% "classe")]
testing.scaled = predict(model.scale, testing.model)
testing.pca = predict(model.pca, testing.scaled)
```

Data set used for this assignment was obtained as a subset of original
data [here][1]. Before approaching model
design, descriptive variables (*X*, *user_name*, *raw_timestamp_part_1*, 
*raw_timestamp_part_2*, *cvtd_timestamp*) and records containing aggregate
statistics for larger time window of previous rows in the data set (identified
by variable value `new_window == "yes"`) were removed.  

All remaining variables were tested for completeness, i.e. if there is at least
one valid value in the whole data set. Even this weak criterion and previously
described steps reduced variable column space from 160 to `r colCount`
(resp. from 159 to `r colCount - 1` if counting off `classe`).

Full data set of `r rowCount` variables was divided into train and test sets,
with relative sizes of `r proportionTrain` and `r 1 - proportionTrain` for
both sets respectively.  

For purpose of exploratory analysis and model fitting, three versions of feature
model were prepared:  

  1. **raw** - no transformation, original values  
  2. **normalized** - features were scaled and centered  
  3. **pca** - principal components transformations, preserving 95% of original
  variance

# Data exploration

Before exploration of sample space, we need to assure that our predicted 
variable `classe` (type of the activity being performed) is approximately
distributed or at least doesn't contain any very rare classes:

``` {r echo=FALSE}
summary(training$classe)
```

Now we explore if any of variables are unnecessary in terms of very low
variance. Such variables have very low predictive power and could cause
over-fitting of the model. As you can see on following figure, no such variables
were identified:

```{r echo=FALSE, cache=TRUE, warning=FALSE}
if(!require(caret)) {
  stop("install caret package")
}

nearZeroVar(x = model.mat, saveMetrics = TRUE)
```

Since our feature space is still `r colCount - 1`-dimensional, we will use pca
transformation to get a basic insight into the aggregate information contained
in the data set. As already mentioned in previous text, principal components
were chosen as such to preserve 95% of the variance in the data, which yielded
`r ncol(model.pca.projected)` principal directions. Even projection of data to
the first two of them shows separation of data into 5 subgroups. However, this
discernible pattern does not correspond to a distribution of `classe` variable
pictured by color of the points.

``` {r echo=FALSE, warning=FALSE, cache=TRUE, comment=FALSE, fig.cap="First two principal components"}
if(!(require(RColorBrewer))) {
  stop("install RColorBrewer package")
}

## plot first two principal components
pallete = brewer.pal(length(levels(model.pca.projected$classe)), "Accent")

theme = theme_set(theme_minimal())
ggplot(data = model.pca.projected, aes(x = PC1, y = PC2, col = classe)) +
  geom_jitter(alpha = 0.6) + scale_color_manual(values = pallete)
```

If we involve the third principal component, we could see that clusters from
previous figure are forming oblong clusters on the next figure (third principal
component is on vertical axis).

``` {r echo=FALSE, warning=FALSE, cache=TRUE, comment=FALSE, fig.cap="First three principal components"}
if(!(require(scatterplot3d))) {
  stop("install scatterplot3d package")
}

## plot again with first three
col = pallete[as.numeric(model.pca.projected$classe)]
with(model.pca.projected, 
     scatterplot3d(x = PC1, y = PC2, z = PC3,
                   pch = 20, angle = 100, color = col,
                   xlab = "first principal component",
                   ylab = "second principal component",
                   zlab = "third principal component"))
legend("topright",
       pch = 20, yjust=0,
       legend = levels(model.pca.projected$classe), 
       col = pallete)
```

Since PCA reduced the number of dimensions by 
`r ncol(model.pca.projected) * 100 / (colCount - 1)`% while still preserving 95%
of original variance, we will use principal components transformation in order
to speed up the learning process of our predictive algorithm.

To understand which of the original variables has the greatest influence on
first three principal directions, we can inspect coefficients used to obtain
each of the corresponding principal scores. In the next figure you can see
top 10 most influential coefficients for first principal direction ordered by
their absolute value:

```{r echo=FALSE}
model.pca.prcomp = as.data.frame(model.pca$rotation)
with(model.pca.prcomp, 
     head(model.pca.prcomp[order(abs(PC1), decreasing = TRUE),][, 1:3]), 10)
```

# Prediction model development

Purpose of following paragraphs is to design a best possible model, where best
means with highest prediction **accuracy**.

## Design

```{r echo=FALSE, warning=FALSE, cache=TRUE, results='hide', comment=FALSE, message=FALSE}
if(!(require(doParallel))) {
  stop("install doParallel package")
}

source('../code/prepareTrainFunction.R')

## run model in parallel
cl = makeCluster(detectCores())
registerDoParallel(cl)

# rrlda model
cv.trainControl = prepareTrainFunction(9)
grid.rrlda = data.frame(lambda = c(0.01, 0.1, 0.3, 1, 3, 6, 10, 20, 30),
                  hp = 0.75,
                  penalty = "L2")
fit.rrlda = train(classe ~ .,
                  data = model.pca.projected,
                  method = "rrlda",
                  trControl = cv.trainControl,
                  tuneGrid = grid.rrlda)


# SVM model
cv.trainControl = prepareTrainFunction(13)
grid.svm = data.frame(C = c(0.1, 1, 3, 5, 10, 15, 20, 30, 35, 40, 45, 50, 60))
fit.svm = train(classe ~ .,
                data = model.pca.projected,
                method = "svmRadialCost",
                trControl = cv.trainControl,
                tuneGrid = grid.svm)

stopCluster(cl)
```

In the beginning we started by fitting two types of models, namely:

* Robust Regularized Linear Discriminant Analysis
* Support Vector Machines with RBF (Gaussian) Kernel

The former of those two models represents an algorithm preferring bias over
variance while using only linear fit, the latter one is on the contrary able
to fit more general types of non-linear relationships between variables and
thus might tend to prefer variance. However both can be tuned by user provided
parameters, which can regularize the fit (increasing $\lambda$ for rrlda,
decreasing __C__ for SVM) or improve its flexibility (increasing __C__ for SVM,
decreasing $\lambda$ for rrlda).

## Tuning

For both of the models, adaptive 10-fold cross-validation was used so as to 
further decrease the training time needed and to improve predictive accuracy. 
Adaptive cross-validation is at the time of writing this paper relatively new
approach presented by [Kuhn (2014)][2] which dynamically reduces number of
candidate tuning parameters during cross-validation. Method used to rule-out
futile models used was the default one for _trainControl_ function used by
_caret_, which uses generalized least squares as a measure of each tuning
parameter's value "fruitfulness" and gets rid of any set of tuning parameters
that have significantly worse cv-error, than that of the best model at given
iteration (one-sided hypothesis test on 95% significant level was used to
determine such models). Finally, first 5 cv-iterations were performed with
full set of potential tuning parameters and reduction of candidate parameters
was carried out at every following iteration, using average model error from
all preceding iterations (refer to original paper for more details about
this method).  
Additional tuning parameters used for rrlda:

```{r echo=FALSE}
print(grid.rrlda)
```

The `hp` parameter is specifying proportion of available training examples
which should be used for model fitting (robustness parameter), `penalty` is 
type of the penalty to be used (we used L2 penalty which is more suitable
for environments where most of the predictors - in our cas pca transforms - are
expected to be significant predictors). $\lambda$ is then the regularization
parameter which we tried to find by by cross-validation.  

Additional tuning parameters used for SVM:

```{r echo=FALSE}
print(grid.svm)
```

The only tuning parameter for SVM was regularization constant __C__ 
(using C-svc type of SV). The $\sigma$ for Gaussian kernel was estimated
internally by method `kernlab::sigest` from by cross-validation.

In the next chapter, summarized results for both of the models are provided,
including the top parameters selected by cross-validation for both models.

# Evaluation

Following figure is showing confusion matrix for SVM model with RBF (Gaussian)
kernel comparing model predictions with our testing set of `r nrow(testing)`
observations. Notice `Accuracy` line which is our final estimate for test error
accuracy of the best SVM model (best model had __C__ = `r fit.svm$bestTune`).

``` {r echo=FALSE, warning=FALSE, message=FALSE}
if(!require(caret)) {
  stop("install caret package")
}
confusionMatrix(testing$classe, predict(fit.svm, testing.pca))
```

Next, we assess performance of rrlda model. As you can see
on the following figure, `Accuracy` was not very good even for the best model
with parameters ($\lambda$,hp,penalty) = (`r fit.rrlda$bestTune`).

```{r echo=FALSE, message=FALSE, warning=FALSE}
if(!require(caret)) {
  stop("install caret package")
}
confusionMatrix(testing$classe, predict(fit.rrlda, testing.pca))
```

As you can see, SVM model has superior test error estimate and since its value
is very close to absolute optimum, there is no need for further tuning.  
`Accuracy` estimation for SVM model is also our estimate for out-of-sample
error.

[1]: http://groupware.les.inf.puc-rio.br/har
[2]: http://arxiv.org/pdf/1405.6974v1.pdf "Kuhn (2014)"